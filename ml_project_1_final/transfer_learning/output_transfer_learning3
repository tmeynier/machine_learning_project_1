Sender: LSF System <lsfadmin@lo-s4-041>
Subject: Job 5239335: <python transfer_learning3.py> in cluster <leonhard> Done

Job <python transfer_learning3.py> was submitted from host <lo-login-01> by user <mchevalley> in cluster <leonhard> at Mon Mar 16 16:13:23 2020
Job was executed on host(s) <4*lo-s4-041>, in queue <gpu.24h>, as user <mchevalley> in cluster <leonhard> at Mon Mar 16 16:13:29 2020
</cluster/home/mchevalley> was used as the home directory.
</cluster/home/mchevalley/ml_project1/transfer_learning> was used as the working directory.
Started at Mon Mar 16 16:13:29 2020
Terminated at Mon Mar 16 16:40:03 2020
Results reported at Mon Mar 16 16:40:03 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python transfer_learning3.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   2704.16 sec.
    Max Memory :                                 1239 MB
    Average Memory :                             1211.19 MB
    Total Requested Memory :                     16384.00 MB
    Delta Memory :                               15145.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                39
    Run time :                                   1589 sec.
    Turnaround time :                            1600 sec.

The output (if any) follows:

2020-03-16 16:13:36.388909: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cluster/apps/lsf/10.1/linux3.10-glibc2.17-x86_64/lib:/cluster/apps/gcc-4.8.5/hdf5-1.10.1-z22q436tpg5j5mrbfk4xex6k2dg73tfo/lib:/cluster/apps/gcc-4.8.5/libpng-1.6.27-d6quyddan2eotsexesxufytusf27xxgz/lib:/cluster/apps/gcc-4.8.5/jpeg-9b-psjvucpx7xwkflb3meatuo6ugdtapkex/lib:/cluster/apps/gcc-4.8.5/nccl-2.4.8-1-xpkpjc3dfz3c5knwctwl5vcut2ahfrwe/lib:/cluster/apps/gcc-4.8.5/cudnn-7.6.4-3ddmfyxxx2gkzhrra44ir3h3ls6yeuwe/lib64:/cluster/apps/gcc-4.8.5/cuda-10.1.243-7tq43deb5x4dbs4oe427w4zszno23q6r/lib64:/cluster/apps/gcc-4.8.5/openblas-0.2.19-w25ydbabttcfa6g76gejjkthcm3xcuv3/lib:/cluster/apps/python/3.7.4/x86_64/lib64
2020-03-16 16:13:36.389198: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cluster/apps/lsf/10.1/linux3.10-glibc2.17-x86_64/lib:/cluster/apps/gcc-4.8.5/hdf5-1.10.1-z22q436tpg5j5mrbfk4xex6k2dg73tfo/lib:/cluster/apps/gcc-4.8.5/libpng-1.6.27-d6quyddan2eotsexesxufytusf27xxgz/lib:/cluster/apps/gcc-4.8.5/jpeg-9b-psjvucpx7xwkflb3meatuo6ugdtapkex/lib:/cluster/apps/gcc-4.8.5/nccl-2.4.8-1-xpkpjc3dfz3c5knwctwl5vcut2ahfrwe/lib:/cluster/apps/gcc-4.8.5/cudnn-7.6.4-3ddmfyxxx2gkzhrra44ir3h3ls6yeuwe/lib64:/cluster/apps/gcc-4.8.5/cuda-10.1.243-7tq43deb5x4dbs4oe427w4zszno23q6r/lib64:/cluster/apps/gcc-4.8.5/openblas-0.2.19-w25ydbabttcfa6g76gejjkthcm3xcuv3/lib:/cluster/apps/python/3.7.4/x86_64/lib64
2020-03-16 16:13:36.389213: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2020-03-16 16:13:39.126130: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-03-16 16:13:39.207622: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:0c:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2020-03-16 16:13:39.209367: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-03-16 16:13:39.214544: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-03-16 16:13:39.217786: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-03-16 16:13:39.219053: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-03-16 16:13:39.222701: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-03-16 16:13:39.225192: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-03-16 16:13:39.230973: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-03-16 16:13:39.238738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-03-16 16:13:39.239357: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-03-16 16:13:39.247683: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200040000 Hz
2020-03-16 16:13:39.248202: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x629c9b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-03-16 16:13:39.248223: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-03-16 16:13:39.465992: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x63232a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-03-16 16:13:39.466034: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2020-03-16 16:13:39.470068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:0c:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2020-03-16 16:13:39.470125: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-03-16 16:13:39.470147: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-03-16 16:13:39.470165: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-03-16 16:13:39.470182: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-03-16 16:13:39.470199: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-03-16 16:13:39.470216: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-03-16 16:13:39.470233: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-03-16 16:13:39.477936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-03-16 16:13:39.477995: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-03-16 16:13:39.482376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-03-16 16:13:39.482401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-03-16 16:13:39.482428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-03-16 16:13:39.490763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10481 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:0c:00.0, compute capability: 6.1)
2020-03-16 16:13:43.448317: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
Train on 10476 samples, validate on 1165 samples
Epoch 1/1000
 - 6s - loss: 0.5321 - acc: 0.7279 - val_loss: 0.5043 - val_acc: 0.7562

Epoch 00001: val_acc improved from -inf to 0.75622, saving model to transfert_learning3.h5
Epoch 2/1000
 - 5s - loss: 0.4689 - acc: 0.7799 - val_loss: 0.4901 - val_acc: 0.7536

Epoch 00002: val_acc did not improve from 0.75622
Epoch 3/1000
 - 8s - loss: 0.4565 - acc: 0.7865 - val_loss: 0.4841 - val_acc: 0.7648

Epoch 00003: val_acc improved from 0.75622 to 0.76481, saving model to transfert_learning3.h5
Epoch 4/1000
 - 9s - loss: 0.4504 - acc: 0.7859 - val_loss: 0.4812 - val_acc: 0.7614

Epoch 00004: val_acc did not improve from 0.76481
Epoch 5/1000
 - 9s - loss: 0.4471 - acc: 0.7882 - val_loss: 0.4791 - val_acc: 0.7631

Epoch 00005: val_acc did not improve from 0.76481
Epoch 6/1000
 - 9s - loss: 0.4440 - acc: 0.7869 - val_loss: 0.4793 - val_acc: 0.7614

Epoch 00006: val_acc did not improve from 0.76481

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
Epoch 7/1000
 - 9s - loss: 0.4422 - acc: 0.7897 - val_loss: 0.4769 - val_acc: 0.7682

Epoch 00007: val_acc improved from 0.76481 to 0.76824, saving model to transfert_learning3.h5
Epoch 8/1000
 - 9s - loss: 0.4419 - acc: 0.7901 - val_loss: 0.4766 - val_acc: 0.7700

Epoch 00008: val_acc improved from 0.76824 to 0.76996, saving model to transfert_learning3.h5
Epoch 9/1000
 - 9s - loss: 0.4416 - acc: 0.7903 - val_loss: 0.4764 - val_acc: 0.7708

Epoch 00009: val_acc improved from 0.76996 to 0.77082, saving model to transfert_learning3.h5
Epoch 10/1000
 - 9s - loss: 0.4415 - acc: 0.7917 - val_loss: 0.4767 - val_acc: 0.7725

Epoch 00010: val_acc improved from 0.77082 to 0.77253, saving model to transfert_learning3.h5
Epoch 11/1000
 - 9s - loss: 0.4413 - acc: 0.7905 - val_loss: 0.4764 - val_acc: 0.7725

Epoch 00011: val_acc did not improve from 0.77253
Epoch 12/1000
 - 9s - loss: 0.4410 - acc: 0.7907 - val_loss: 0.4759 - val_acc: 0.7682

Epoch 00012: val_acc did not improve from 0.77253
Epoch 13/1000
 - 9s - loss: 0.4409 - acc: 0.7900 - val_loss: 0.4763 - val_acc: 0.7717

Epoch 00013: val_acc did not improve from 0.77253

Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
Epoch 14/1000
 - 9s - loss: 0.4406 - acc: 0.7904 - val_loss: 0.4763 - val_acc: 0.7717

Epoch 00014: val_acc did not improve from 0.77253
Epoch 15/1000
 - 9s - loss: 0.4406 - acc: 0.7906 - val_loss: 0.4762 - val_acc: 0.7717

Epoch 00015: val_acc did not improve from 0.77253
Epoch 16/1000
 - 9s - loss: 0.4406 - acc: 0.7902 - val_loss: 0.4762 - val_acc: 0.7717

Epoch 00016: val_acc did not improve from 0.77253

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
Epoch 17/1000
 - 8s - loss: 0.4406 - acc: 0.7903 - val_loss: 0.4762 - val_acc: 0.7717

Epoch 00017: val_acc did not improve from 0.77253
Epoch 18/1000
 - 9s - loss: 0.4406 - acc: 0.7904 - val_loss: 0.4762 - val_acc: 0.7717

Epoch 00018: val_acc did not improve from 0.77253
Epoch 19/1000
 - 9s - loss: 0.4406 - acc: 0.7902 - val_loss: 0.4762 - val_acc: 0.7717

Epoch 00019: val_acc did not improve from 0.77253

Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.
Epoch 20/1000
 - 9s - loss: 0.4406 - acc: 0.7903 - val_loss: 0.4762 - val_acc: 0.7717

Epoch 00020: val_acc did not improve from 0.77253
Epoch 00020: early stopping
Train on 10476 samples, validate on 1165 samples
Epoch 1/1000
 - 34s - loss: 0.4754 - acc: 0.7512 - val_loss: 0.4184 - val_acc: 0.7751

Epoch 00001: val_acc improved from 0.77253 to 0.77511, saving model to transfert_learning3.h5
Epoch 2/1000
 - 33s - loss: 0.4317 - acc: 0.7739 - val_loss: 0.4512 - val_acc: 0.7571

Epoch 00002: val_acc did not improve from 0.77511
Epoch 3/1000
 - 33s - loss: 0.4096 - acc: 0.7868 - val_loss: 0.4207 - val_acc: 0.8052

Epoch 00003: val_acc improved from 0.77511 to 0.80515, saving model to transfert_learning3.h5
Epoch 4/1000
 - 33s - loss: 0.4057 - acc: 0.7961 - val_loss: 0.4415 - val_acc: 0.7691

Epoch 00004: val_acc did not improve from 0.80515
Epoch 5/1000
 - 33s - loss: 0.4215 - acc: 0.7821 - val_loss: 0.4193 - val_acc: 0.7734

Epoch 00005: val_acc did not improve from 0.80515
Epoch 6/1000
 - 33s - loss: 0.3837 - acc: 0.8035 - val_loss: 0.3924 - val_acc: 0.8172

Epoch 00006: val_acc improved from 0.80515 to 0.81717, saving model to transfert_learning3.h5
Epoch 7/1000
 - 33s - loss: 0.3930 - acc: 0.8021 - val_loss: 0.3800 - val_acc: 0.8060

Epoch 00007: val_acc did not improve from 0.81717
Epoch 8/1000
 - 33s - loss: 0.3878 - acc: 0.8009 - val_loss: 0.4140 - val_acc: 0.7777

Epoch 00008: val_acc did not improve from 0.81717
Epoch 9/1000
 - 34s - loss: 0.3814 - acc: 0.8136 - val_loss: 0.4094 - val_acc: 0.8077

Epoch 00009: val_acc did not improve from 0.81717

Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.
Epoch 10/1000
 - 33s - loss: 0.3392 - acc: 0.8352 - val_loss: 0.3455 - val_acc: 0.8395

Epoch 00010: val_acc improved from 0.81717 to 0.83948, saving model to transfert_learning3.h5
Epoch 11/1000
 - 33s - loss: 0.3199 - acc: 0.8532 - val_loss: 0.3418 - val_acc: 0.8421

Epoch 00011: val_acc improved from 0.83948 to 0.84206, saving model to transfert_learning3.h5
Epoch 12/1000
 - 32s - loss: 0.3125 - acc: 0.8545 - val_loss: 0.3263 - val_acc: 0.8549

Epoch 00012: val_acc improved from 0.84206 to 0.85494, saving model to transfert_learning3.h5
Epoch 13/1000
 - 33s - loss: 0.3052 - acc: 0.8640 - val_loss: 0.3126 - val_acc: 0.8609

Epoch 00013: val_acc improved from 0.85494 to 0.86094, saving model to transfert_learning3.h5
Epoch 14/1000
 - 33s - loss: 0.3007 - acc: 0.8647 - val_loss: 0.3088 - val_acc: 0.8609

Epoch 00014: val_acc did not improve from 0.86094
Epoch 15/1000
 - 33s - loss: 0.2928 - acc: 0.8713 - val_loss: 0.3000 - val_acc: 0.8721

Epoch 00015: val_acc improved from 0.86094 to 0.87210, saving model to transfert_learning3.h5
Epoch 16/1000
 - 33s - loss: 0.2853 - acc: 0.8726 - val_loss: 0.2950 - val_acc: 0.8730

Epoch 00016: val_acc improved from 0.87210 to 0.87296, saving model to transfert_learning3.h5
Epoch 17/1000
 - 33s - loss: 0.2817 - acc: 0.8763 - val_loss: 0.2938 - val_acc: 0.8670

Epoch 00017: val_acc did not improve from 0.87296
Epoch 18/1000
 - 32s - loss: 0.2724 - acc: 0.8861 - val_loss: 0.2909 - val_acc: 0.8781

Epoch 00018: val_acc improved from 0.87296 to 0.87811, saving model to transfert_learning3.h5
Epoch 19/1000
 - 33s - loss: 0.2569 - acc: 0.8933 - val_loss: 0.2815 - val_acc: 0.8850

Epoch 00019: val_acc improved from 0.87811 to 0.88498, saving model to transfert_learning3.h5
Epoch 20/1000
 - 33s - loss: 0.2478 - acc: 0.8953 - val_loss: 0.2766 - val_acc: 0.8781

Epoch 00020: val_acc did not improve from 0.88498
Epoch 21/1000
 - 32s - loss: 0.2407 - acc: 0.9010 - val_loss: 0.2682 - val_acc: 0.8876

Epoch 00021: val_acc improved from 0.88498 to 0.88755, saving model to transfert_learning3.h5
Epoch 22/1000
 - 32s - loss: 0.2374 - acc: 0.8998 - val_loss: 0.2700 - val_acc: 0.8824

Epoch 00022: val_acc did not improve from 0.88755
Epoch 23/1000
 - 32s - loss: 0.2326 - acc: 0.9026 - val_loss: 0.2749 - val_acc: 0.8833

Epoch 00023: val_acc did not improve from 0.88755
Epoch 24/1000
 - 32s - loss: 0.2286 - acc: 0.9051 - val_loss: 0.2534 - val_acc: 0.8918

Epoch 00024: val_acc improved from 0.88755 to 0.89185, saving model to transfert_learning3.h5
Epoch 25/1000
 - 32s - loss: 0.2193 - acc: 0.9086 - val_loss: 0.2653 - val_acc: 0.8815

Epoch 00025: val_acc did not improve from 0.89185
Epoch 26/1000
 - 33s - loss: 0.2217 - acc: 0.9077 - val_loss: 0.2567 - val_acc: 0.8858

Epoch 00026: val_acc did not improve from 0.89185
Epoch 27/1000
 - 33s - loss: 0.2141 - acc: 0.9097 - val_loss: 0.2490 - val_acc: 0.9004

Epoch 00027: val_acc improved from 0.89185 to 0.90043, saving model to transfert_learning3.h5
Epoch 28/1000
 - 32s - loss: 0.2130 - acc: 0.9149 - val_loss: 0.2378 - val_acc: 0.8961

Epoch 00028: val_acc did not improve from 0.90043
Epoch 29/1000
 - 33s - loss: 0.2057 - acc: 0.9153 - val_loss: 0.2430 - val_acc: 0.8953

Epoch 00029: val_acc did not improve from 0.90043
Epoch 30/1000
 - 32s - loss: 0.2094 - acc: 0.9149 - val_loss: 0.2622 - val_acc: 0.8901

Epoch 00030: val_acc did not improve from 0.90043

Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.
Epoch 31/1000
 - 32s - loss: 0.1877 - acc: 0.9256 - val_loss: 0.2283 - val_acc: 0.9039

Epoch 00031: val_acc improved from 0.90043 to 0.90386, saving model to transfert_learning3.h5
Epoch 32/1000
 - 33s - loss: 0.1838 - acc: 0.9271 - val_loss: 0.2317 - val_acc: 0.9030

Epoch 00032: val_acc did not improve from 0.90386
Epoch 33/1000
 - 33s - loss: 0.1824 - acc: 0.9262 - val_loss: 0.2292 - val_acc: 0.9047

Epoch 00033: val_acc improved from 0.90386 to 0.90472, saving model to transfert_learning3.h5
Epoch 34/1000
 - 32s - loss: 0.1813 - acc: 0.9273 - val_loss: 0.2314 - val_acc: 0.9021

Epoch 00034: val_acc did not improve from 0.90472
Epoch 35/1000
 - 33s - loss: 0.1814 - acc: 0.9268 - val_loss: 0.2289 - val_acc: 0.9030

Epoch 00035: val_acc did not improve from 0.90472
Epoch 36/1000
 - 32s - loss: 0.1804 - acc: 0.9281 - val_loss: 0.2273 - val_acc: 0.9004

Epoch 00036: val_acc did not improve from 0.90472

Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.
Epoch 37/1000
 - 33s - loss: 0.1785 - acc: 0.9292 - val_loss: 0.2277 - val_acc: 0.9030

Epoch 00037: val_acc did not improve from 0.90472
Epoch 38/1000
 - 33s - loss: 0.1782 - acc: 0.9285 - val_loss: 0.2279 - val_acc: 0.9030

Epoch 00038: val_acc did not improve from 0.90472
Epoch 39/1000
 - 33s - loss: 0.1781 - acc: 0.9287 - val_loss: 0.2280 - val_acc: 0.9039

Epoch 00039: val_acc did not improve from 0.90472

Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.
Epoch 40/1000
 - 33s - loss: 0.1779 - acc: 0.9285 - val_loss: 0.2280 - val_acc: 0.9039

Epoch 00040: val_acc did not improve from 0.90472
Epoch 41/1000
 - 32s - loss: 0.1778 - acc: 0.9287 - val_loss: 0.2280 - val_acc: 0.9039

Epoch 00041: val_acc did not improve from 0.90472
Epoch 42/1000
 - 32s - loss: 0.1778 - acc: 0.9285 - val_loss: 0.2279 - val_acc: 0.9030

Epoch 00042: val_acc did not improve from 0.90472

Epoch 00042: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.
Epoch 43/1000
 - 33s - loss: 0.1778 - acc: 0.9287 - val_loss: 0.2279 - val_acc: 0.9030
Using TensorFlow backend.
transfer_learning3.py:25: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor("de...)`
  frozen_model = Model(inputs = new_model.inputs, output = layer)

Epoch 00043: val_acc did not improve from 0.90472
Epoch 00043: early stopping
Test f1 score : 0.8910774020220464 
Test accuracy score : 0.9124012366884232 
